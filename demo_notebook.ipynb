{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc97ac13",
   "metadata": {},
   "source": [
    "# Folder RAG - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the core functionality of the Folder RAG project:\n",
    "- Database operations\n",
    "- Japanese tokenization with MeCab\n",
    "- Full-text search with FTS5\n",
    "- Configuration management\n",
    "\n",
    "**Phase 1 Complete** ‚úì\n",
    "\n",
    "Run each cell in order to explore the features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc5157",
   "metadata": {},
   "source": [
    "## 1. Setup - Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc696599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All modules imported successfully!\n",
      "‚úì Python version: 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "from src.core.database import Database\n",
    "from src.core.tokenizer import get_tokenizer\n",
    "from src.core.config import Config\n",
    "from src.core.models import Document, Chunk, DocumentStatus\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "print(\"‚úì All modules imported successfully!\")\n",
    "print(f\"‚úì Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b51b06",
   "metadata": {},
   "source": [
    "## 2. Test Japanese Tokenizer\n",
    "\n",
    "MeCab automatically splits Japanese text into words for better search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab1de825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MeCab is available and initialized\n",
      "\n",
      "======================================================================\n",
      "Original:  Ê©üÊ¢∞Â≠¶Áøí„ÅØPython„ÅßÂÆüË£Ö„Åß„Åç„Åæ„Åô\n",
      "Tokenized: Ê©üÊ¢∞ Â≠¶Áøí „ÅØ Python „Åß ÂÆüË£Ö „Åß„Åç „Åæ„Åô\n",
      "Count:     8 tokens\n",
      "----------------------------------------------------------------------\n",
      "Original:  Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Å®RAG„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥\n",
      "Tokenized: Ëá™ÁÑ∂ Ë®ÄË™û Âá¶ÁêÜ „Å® RAG „Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥\n",
      "Count:     6 tokens\n",
      "----------------------------------------------------------------------\n",
      "Original:  Ê∑±Â±§Â≠¶Áøí„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅAIÊäÄË°ì\n",
      "Tokenized: Ê∑±Â±§ Â≠¶Áøí „ÄÅ „Éã„É•„Éº„É©„É´ „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ „ÄÅ AI ÊäÄË°ì\n",
      "Count:     8 tokens\n",
      "----------------------------------------------------------------------\n",
      "Original:  Python is a programming language\n",
      "Tokenized: Python is a programming language\n",
      "Count:     5 tokens\n",
      "----------------------------------------------------------------------\n",
      "Original:  Êó•Êú¨Ë™û„Å®English„ÅÆÊ∑∑Âú®„ÉÜ„Ç≠„Çπ„Éà\n",
      "Tokenized: Êó•Êú¨ Ë™û „Å® English „ÅÆ Ê∑∑Âú® „ÉÜ„Ç≠„Çπ„Éà\n",
      "Count:     7 tokens\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Check if MeCab is available\n",
    "if tokenizer.mecab:\n",
    "    print(\"‚úì MeCab is available and initialized\\n\")\n",
    "else:\n",
    "    print(\"‚úó MeCab not available, using fallback\\n\")\n",
    "\n",
    "# Test Japanese tokenization\n",
    "test_texts = [\n",
    "    \"Ê©üÊ¢∞Â≠¶Áøí„ÅØPython„ÅßÂÆüË£Ö„Åß„Åç„Åæ„Åô\",\n",
    "    \"Ëá™ÁÑ∂Ë®ÄË™ûÂá¶ÁêÜ„Å®RAG„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥\",\n",
    "    \"Ê∑±Â±§Â≠¶Áøí„ÄÅ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„ÄÅAIÊäÄË°ì\",\n",
    "    \"Python is a programming language\",\n",
    "    \"Êó•Êú¨Ë™û„Å®English„ÅÆÊ∑∑Âú®„ÉÜ„Ç≠„Çπ„Éà\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "for text in test_texts:\n",
    "    print(f\"Original:  {text}\")\n",
    "    tokenized = tokenizer.tokenize(text)\n",
    "    print(f\"Tokenized: {tokenized}\")\n",
    "    tokens = tokenizer.get_tokens_list(text)\n",
    "    print(f\"Count:     {len(tokens)} tokens\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f63db5",
   "metadata": {},
   "source": [
    "## 3. Initialize Database\n",
    "\n",
    "Create a database instance and verify the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6215120c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Database initialized\n",
      "‚úì Database path: data\\folderrag.db\n",
      "‚úì Current chunk count: 0\n",
      "‚úì Current document count: 0\n"
     ]
    }
   ],
   "source": [
    "# Create database (uses data/folderrag.db)\n",
    "db = Database()\n",
    "\n",
    "print(\"‚úì Database initialized\")\n",
    "print(f\"‚úì Database path: {db.db_path}\")\n",
    "print(f\"‚úì Current chunk count: {db.get_chunk_count()}\")\n",
    "print(f\"‚úì Current document count: {len(db.get_all_documents())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86da6f0",
   "metadata": {},
   "source": [
    "## 4. Add Test Documents\n",
    "\n",
    "Let's add some sample documents with Japanese and English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85b09a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Added 1 test documents:\n",
      "  1. „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create test documents\n",
    "doc1 = Document(\n",
    "    id=str(uuid.uuid4()),\n",
    "    path=\"C:\\\\Development\\\\projects\\\\myRAG\\\\test.pdf\",\n",
    "    title=\"„Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\",\n",
    "    ext=\".pdf\",\n",
    "    mtime=datetime.now(),\n",
    "    size=2048,\n",
    "    status=DocumentStatus.INDEXED\n",
    ")\n",
    "\n",
    "# doc2 = Document(\n",
    "#     id=str(uuid.uuid4()),\n",
    "#     path=\"C:/test/python_guide.pdf\",\n",
    "#     title=\"Python Programming Guide.pdf\",\n",
    "#     ext=\".pdf\",\n",
    "#     mtime=datetime.now(),\n",
    "#     size=3072,\n",
    "#     status=DocumentStatus.INDEXED\n",
    "# )\n",
    "\n",
    "# Add to database\n",
    "db.add_document(doc1)\n",
    "# db.add_document(doc2)\n",
    "\n",
    "print(\"‚úì Added 1 test documents:\")\n",
    "print(f\"  1. {doc1.title}\")\n",
    "# print(f\"  2. {doc2.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5fdc43",
   "metadata": {},
   "source": [
    "## 5. Add Text Chunks\n",
    "\n",
    "Add text chunks in Japanese and English. Note how Japanese text is automatically tokenized!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97f78ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Japanese chunks...\n",
      "  ‚úì Chunk 1: Ê©üÊ¢∞Â≠¶Áøí„ÅØ‰∫∫Â∑•Áü•ËÉΩ„ÅÆ‰∏ÄÂàÜÈáé„Åß„Åô„ÄÇ„Éá„Éº„Çø„Åã„Çâ„Éë„Çø„Éº„É≥„ÇíÂ≠¶Áøí„Åó„Åæ...\n",
      "  ‚úì Chunk 2: ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„ÄÅÊïôÂ∏´„Å™„ÅóÂ≠¶Áøí„ÄÅÂº∑ÂåñÂ≠¶Áøí„ÅÆ‰∏â„Å§„ÅÆ‰∏ªË¶Å„Å™Â≠¶ÁøíÊñπÊ≥ï„Åå...\n",
      "  ‚úì Chunk 3: Python„ÅØÊ©üÊ¢∞Â≠¶Áøí„ÅßÊúÄ„ÇÇ‰∫∫Ê∞ó„ÅÆ„ÅÇ„Çã„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„Åß„Åô...\n",
      "  ‚úì Chunk 4: Ê∑±Â±§Â≠¶Áøí„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Çí‰ΩøÁî®„Åó„ÅüÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊâãÊ≥ï„Åß„Åô...\n",
      "\n",
      "Adding English chunks...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'doc2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAdding English chunks...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(english_chunks):\n\u001b[32m     35\u001b[39m     chunk = Chunk(\n\u001b[32m     36\u001b[39m         \u001b[38;5;28mid\u001b[39m=\u001b[38;5;28mstr\u001b[39m(uuid.uuid4()),\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m         document_id=\u001b[43mdoc2\u001b[49m.id,\n\u001b[32m     38\u001b[39m         page=i + \u001b[32m1\u001b[39m,\n\u001b[32m     39\u001b[39m         start_offset=i * \u001b[32m100\u001b[39m,\n\u001b[32m     40\u001b[39m         end_offset=(i + \u001b[32m1\u001b[39m) * \u001b[32m100\u001b[39m,\n\u001b[32m     41\u001b[39m         text=text,\n\u001b[32m     42\u001b[39m         text_hash=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhash_en_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     43\u001b[39m     )\n\u001b[32m     44\u001b[39m     db.add_chunk(chunk)\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚úì Chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext[:\u001b[32m50\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'doc2' is not defined"
     ]
    }
   ],
   "source": [
    "# Japanese chunks for doc1\n",
    "japanese_chunks = [\n",
    "    \"Ê©üÊ¢∞Â≠¶Áøí„ÅØ‰∫∫Â∑•Áü•ËÉΩ„ÅÆ‰∏ÄÂàÜÈáé„Åß„Åô„ÄÇ„Éá„Éº„Çø„Åã„Çâ„Éë„Çø„Éº„É≥„ÇíÂ≠¶Áøí„Åó„Åæ„Åô„ÄÇ\",\n",
    "    \"ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶Áøí„ÄÅÊïôÂ∏´„Å™„ÅóÂ≠¶Áøí„ÄÅÂº∑ÂåñÂ≠¶Áøí„ÅÆ‰∏â„Å§„ÅÆ‰∏ªË¶Å„Å™Â≠¶ÁøíÊñπÊ≥ï„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\",\n",
    "    \"Python„ÅØÊ©üÊ¢∞Â≠¶Áøí„ÅßÊúÄ„ÇÇ‰∫∫Ê∞ó„ÅÆ„ÅÇ„Çã„Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞Ë®ÄË™û„Åß„Åô„ÄÇ\",\n",
    "    \"Ê∑±Â±§Â≠¶Áøí„ÅØ„Éã„É•„Éº„É©„É´„Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Çí‰ΩøÁî®„Åó„ÅüÊ©üÊ¢∞Â≠¶Áøí„ÅÆÊâãÊ≥ï„Åß„Åô„ÄÇ\",\n",
    "]\n",
    "\n",
    "# English chunks for doc2\n",
    "english_chunks = [\n",
    "    \"Python is a high-level programming language known for its simplicity.\",\n",
    "    \"Machine learning libraries like scikit-learn and TensorFlow are popular.\",\n",
    "    \"Python supports multiple programming paradigms including OOP and functional.\",\n",
    "    \"Data science and artificial intelligence applications often use Python.\",\n",
    "]\n",
    "\n",
    "# Add Japanese chunks\n",
    "print(\"Adding Japanese chunks...\")\n",
    "for i, text in enumerate(japanese_chunks):\n",
    "    chunk = Chunk(\n",
    "        id=str(uuid.uuid4()),\n",
    "        document_id=doc1.id,\n",
    "        page=i + 1,\n",
    "        start_offset=i * 100,\n",
    "        end_offset=(i + 1) * 100,\n",
    "        text=text,\n",
    "        text_hash=f\"hash_jp_{i}\"\n",
    "    )\n",
    "    db.add_chunk(chunk)\n",
    "    print(f\"  ‚úì Chunk {i+1}: {text[:30]}...\")\n",
    "\n",
    "# Add English chunks\n",
    "print(\"\\nAdding English chunks...\")\n",
    "for i, text in enumerate(english_chunks):\n",
    "    chunk = Chunk(\n",
    "        id=str(uuid.uuid4()),\n",
    "        document_id=doc2.id,\n",
    "        page=i + 1,\n",
    "        start_offset=i * 100,\n",
    "        end_offset=(i + 1) * 100,\n",
    "        text=text,\n",
    "        text_hash=f\"hash_en_{i}\"\n",
    "    )\n",
    "    db.add_chunk(chunk)\n",
    "    print(f\"  ‚úì Chunk {i+1}: {text[:50]}...\")\n",
    "\n",
    "print(f\"\\n‚úì Total chunks in database: {db.get_chunk_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e41f7d2",
   "metadata": {},
   "source": [
    "## 6. Search Test - Japanese Keywords\n",
    "\n",
    "Now let's test FTS5 search with Japanese keywords. Thanks to MeCab, we can search for individual words!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5b944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Search Query: 'Â≠¶Áøí'\n",
      "======================================================================\n",
      "Found 4 results:\n",
      "\n",
      "1. Score: -0.0000\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 2\n",
      "   Text: ÊïôÂ∏´ „ÅÇ„Çä Â≠¶Áøí „ÄÅ ÊïôÂ∏´ „Å™„Åó Â≠¶Áøí „ÄÅ Âº∑Âåñ Â≠¶Áøí „ÅÆ ‰∏â „Å§ „ÅÆ ‰∏ªË¶Å „Å™ Â≠¶Áøí ÊñπÊ≥ï „Åå „ÅÇ„Çä „Åæ„Åô „ÄÇ\n",
      "\n",
      "2. Score: -0.0000\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 4\n",
      "   Text: Ê∑±Â±§ Â≠¶Áøí „ÅØ „Éã„É•„Éº„É©„É´ „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ „Çí ‰ΩøÁî® „Åó „Åü Ê©üÊ¢∞ Â≠¶Áøí „ÅÆ ÊâãÊ≥ï „Åß„Åô „ÄÇ\n",
      "\n",
      "3. Score: -0.0000\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 1\n",
      "   Text: Ê©üÊ¢∞ Â≠¶Áøí „ÅØ ‰∫∫Â∑• Áü•ËÉΩ „ÅÆ ‰∏Ä ÂàÜÈáé „Åß„Åô „ÄÇ „Éá„Éº„Çø „Åã„Çâ „Éë„Çø„Éº„É≥ „Çí Â≠¶Áøí „Åó „Åæ„Åô „ÄÇ\n",
      "\n",
      "4. Score: -0.0000\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 3\n",
      "   Text: Python „ÅØ Ê©üÊ¢∞ Â≠¶Áøí „Åß ÊúÄ„ÇÇ ‰∫∫Ê∞ó „ÅÆ „ÅÇ„Çã „Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞ Ë®ÄË™û „Åß„Åô „ÄÇ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Search Query: 'ÊïôÂ∏´„Å™„Åó'\n",
      "======================================================================\n",
      "Found 1 results:\n",
      "\n",
      "1. Score: -1.8595\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 2\n",
      "   Text: ÊïôÂ∏´ „ÅÇ„Çä Â≠¶Áøí „ÄÅ ÊïôÂ∏´ „Å™„Åó Â≠¶Áøí „ÄÅ Âº∑Âåñ Â≠¶Áøí „ÅÆ ‰∏â „Å§ „ÅÆ ‰∏ªË¶Å „Å™ Â≠¶Áøí ÊñπÊ≥ï „Åå „ÅÇ„Çä „Åæ„Åô „ÄÇ\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Search Query: 'Python'\n",
      "======================================================================\n",
      "Found 1 results:\n",
      "\n",
      "1. Score: -0.9282\n",
      "   Document: „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "   Page: 3\n",
      "   Text: Python „ÅØ Ê©üÊ¢∞ Â≠¶Áøí „Åß ÊúÄ„ÇÇ ‰∫∫Ê∞ó „ÅÆ „ÅÇ„Çã „Éó„É≠„Ç∞„É©„Éü„É≥„Ç∞ Ë®ÄË™û „Åß„Åô „ÄÇ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_and_display(query, limit=5):\n",
    "    \"\"\"Helper function to search and display results.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Search Query: '{query}'\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = db.search_chunks_fts(query, limit=limit)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No results found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(results)} results:\\n\")\n",
    "    \n",
    "    for i, (chunk_id, score) in enumerate(results, 1):\n",
    "        chunk = db.get_chunk(chunk_id)\n",
    "        doc = db.get_document(chunk.document_id)\n",
    "        print(f\"{i}. Score: {score:.4f}\")\n",
    "        print(f\"   Document: {doc.title}\")\n",
    "        print(f\"   Page: {chunk.page}\")\n",
    "        print(f\"   Text: {chunk.text}\")\n",
    "        print()\n",
    "\n",
    "# Test Japanese searches\n",
    "search_and_display(\"Â≠¶Áøí\")\n",
    "search_and_display(\"ÊïôÂ∏´„Å™„Åó\")\n",
    "search_and_display(\"Python\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03571143",
   "metadata": {},
   "source": [
    "## 7. Search Test - English Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "296c3785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Search Query: 'programming'\n",
      "======================================================================\n",
      "No results found.\n",
      "\n",
      "======================================================================\n",
      "Search Query: 'scikit-learn'\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "no such column: learn",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m search_and_display(\u001b[33m\"\u001b[39m\u001b[33mprogramming\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msearch_and_display\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m search_and_display(\u001b[33m\"\u001b[39m\u001b[33martificial intelligence\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36msearch_and_display\u001b[39m\u001b[34m(query, limit)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSearch Query: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mdb\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_chunks_fts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo results found.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Development\\projects\\myRAG\\src\\core\\database.py:334\u001b[39m, in \u001b[36mDatabase.search_chunks_fts\u001b[39m\u001b[34m(self, query, limit)\u001b[39m\n\u001b[32m    331\u001b[39m tokenized_query = \u001b[38;5;28mself\u001b[39m.tokenizer.tokenize(query)\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_connection() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     rows = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m    335\u001b[39m \u001b[33;43m        SELECT c.id as chunk_id, cf.rank as score\u001b[39;49m\n\u001b[32m    336\u001b[39m \u001b[33;43m        FROM chunks_fts cf\u001b[39;49m\n\u001b[32m    337\u001b[39m \u001b[33;43m        JOIN chunks c ON c.rowid = cf.rowid\u001b[39;49m\n\u001b[32m    338\u001b[39m \u001b[33;43m        WHERE chunks_fts MATCH ?\u001b[39;49m\n\u001b[32m    339\u001b[39m \u001b[33;43m        ORDER BY cf.rank\u001b[39;49m\n\u001b[32m    340\u001b[39m \u001b[33;43m        LIMIT ?\u001b[39;49m\n\u001b[32m    341\u001b[39m \u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.fetchall()\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [(row[\u001b[33m'\u001b[39m\u001b[33mchunk_id\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows]\n",
      "\u001b[31mOperationalError\u001b[39m: no such column: learn"
     ]
    }
   ],
   "source": [
    "search_and_display(\"programming\")\n",
    "search_and_display(\"scikit-learn\")\n",
    "search_and_display(\"artificial intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511713d",
   "metadata": {},
   "source": [
    "## 8. Configuration Management\n",
    "\n",
    "View and modify application settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(db)\n",
    "settings = config.get_settings()\n",
    "\n",
    "print(\"Current Settings:\")\n",
    "print(f\"  Chunk size: {settings.chunk_size} tokens\")\n",
    "print(f\"  Chunk overlap: {settings.chunk_overlap} tokens\")\n",
    "print(f\"  Top K results: {settings.top_k}\")\n",
    "print(f\"  Allowed extensions: {', '.join(settings.allowed_ext)}\")\n",
    "print(f\"  Embedding model: {settings.embedding_model}\")\n",
    "print(f\"  Generation mode: {settings.generation_mode.value}\")\n",
    "print(f\"  Included paths: {settings.included_paths if settings.included_paths else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d6f5f",
   "metadata": {},
   "source": [
    "## 9. Summary & Statistics\n",
    "\n",
    "Let's see what we've created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf270586",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs = db.get_all_documents()\n",
    "total_chunks = db.get_chunk_count()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATABASE STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal Documents: {len(all_docs)}\")\n",
    "print(f\"Total Chunks: {total_chunks}\")\n",
    "print(f\"\\nDocuments:\")\n",
    "for i, doc in enumerate(all_docs, 1):\n",
    "    chunks = db.get_chunks_by_document(doc.id)\n",
    "    print(f\"  {i}. {doc.title}\")\n",
    "    print(f\"     - Status: {doc.status.value}\")\n",
    "    print(f\"     - Chunks: {len(chunks)}\")\n",
    "    print(f\"     - Size: {doc.size} bytes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì Phase 1 Demo Complete!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhat's working:\")\n",
    "print(\"  ‚úì Japanese tokenization with MeCab\")\n",
    "print(\"  ‚úì Full-text search with FTS5\")\n",
    "print(\"  ‚úì Database operations\")\n",
    "print(\"  ‚úì Configuration management\")\n",
    "print(\"\\nComing in Phase 2:\")\n",
    "print(\"  ‚Ä¢ Automatic PDF/TXT file indexing\")\n",
    "print(\"  ‚Ä¢ Folder scanning\")\n",
    "print(\"  ‚Ä¢ Progress tracking\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f8911c",
   "metadata": {},
   "source": [
    "# Phase 2: File Indexing Pipeline\n",
    "\n",
    "Now let's demonstrate the complete indexing pipeline that was built in Phase 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ded24",
   "metadata": {},
   "source": [
    "## 1. File Ingestion - Scan Folders for Documents\n",
    "\n",
    "The ingestion module scans folders and finds all PDF, TXT, and MD files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad05e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Scanning test data folder...\n",
      "\n",
      "‚úÖ Added: 3 files\n",
      "üîÑ Updated: 0 files\n",
      "‚ùå Errors: 0\n",
      "\n",
      "üìã Pending documents:\n",
      "  - sample.md (.md) - 686 bytes\n",
      "  - sample.pdf (.pdf) - 1910 bytes\n",
      "  - sample.txt (.txt) - 613 bytes\n"
     ]
    }
   ],
   "source": [
    "from src.indexing.ingestion import Ingestion\n",
    "\n",
    "# Create ingestion with our database\n",
    "ingestion = Ingestion(db)\n",
    "\n",
    "# Scan the test data folder\n",
    "print(\"üìÅ Scanning test data folder...\")\n",
    "added, updated, errors = ingestion.scan_and_add('tests/test_data', recursive=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Added: {added} files\")\n",
    "print(f\"üîÑ Updated: {updated} files\")\n",
    "print(f\"‚ùå Errors: {len(errors)}\")\n",
    "\n",
    "if errors:\n",
    "    for error in errors:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "# Show what was found\n",
    "print(f\"\\nüìã Pending documents:\")\n",
    "pending = ingestion.get_pending_documents()\n",
    "for doc in pending:\n",
    "    print(f\"  - {doc.title} ({doc.ext}) - {doc.size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048e2e3",
   "metadata": {},
   "source": [
    "## 2. Text Extraction - Extract from PDF, TXT, MD\n",
    "\n",
    "Let's extract text from one of the documents to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd06ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting text from: sample.md\n",
      "\n",
      "üìä Extraction Results:\n",
      "  - Total pages: 1\n",
      "  - Total characters: 549\n",
      "\n",
      "üìñ Page-by-page content:\n",
      "\n",
      "  Page 1 (549 chars):\n",
      "    # Test Markdown File  This is a test markdown file for myRAG.  ## Section 1: Introduction  This document contains **formatted text** with _italics_ an...\n",
      "\n",
      "üìù Full text of page 1:\n",
      "# Test Markdown File\n",
      "\n",
      "This is a test markdown file for myRAG.\n",
      "\n",
      "## Section 1: Introduction\n",
      "\n",
      "This document contains **formatted text** with _italics_ and other markdown features.\n",
      "\n",
      "## Section 2: Japanese Content\n",
      "\n",
      "Êó•Êú¨Ë™û„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇÇ„ÉÜ„Çπ„Éà„Åó„Åæ„Åô„ÄÇ\n",
      "\n",
      "### Subsection 2.1\n",
      "\n",
      "Ê©üÊ¢∞Â≠¶Áøí„Å®„ÅØ„ÄÅ„Ç≥„É≥„Éî„É•„Éº„Çø„Åå„Éá„Éº„Çø„Åã„ÇâÂ≠¶Áøí„Åô„Çã„Ç¢„É´„Ç¥„É™„Ç∫„É†„Åß„Åô„ÄÇ\n",
      "\n",
      "## Section 3: Code Example\n",
      "\n",
      "```python\n",
      "def hello_world():\n",
      "    print(\"Hello, RAG!\")\n",
      "```\n",
      "\n",
      "## Section 4: Lists\n",
      "\n",
      "- Item 1\n",
      "- Item 2\n",
      "- Item 3\n",
      "\n",
      "### Ordered List\n",
      "\n",
      "1. First item\n",
      "2. Second item\n",
      "3. Third item\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "This markdown file tests various formatting features.\n"
     ]
    }
   ],
   "source": [
    "from src.indexing.extractor import Extractor\n",
    "\n",
    "extractor = Extractor()\n",
    "\n",
    "# Extract from the PDF file\n",
    "if pending:\n",
    "    # Get the first pending document\n",
    "    test_doc = pending[0]\n",
    "    print(f\"üìÑ Extracting text from: {test_doc.title}\\n\")\n",
    "    \n",
    "    extracted = extractor.extract(test_doc.path)\n",
    "    \n",
    "    print(f\"üìä Extraction Results:\")\n",
    "    print(f\"  - Total pages: {len(extracted.pages)}\")\n",
    "    print(f\"  - Total characters: {extracted.total_chars}\")\n",
    "    \n",
    "    print(f\"\\nüìñ Page-by-page content:\")\n",
    "    for page in extracted.pages:\n",
    "        print(f\"\\n  Page {page.page_number} ({page.char_count} chars):\")\n",
    "        # Show first 150 characters of each page\n",
    "        preview = page.text[:150].replace('\\n', ' ')\n",
    "        print(f\"    {preview}...\")\n",
    "        \n",
    "    # Show full text of first page\n",
    "    print(f\"\\nüìù Full text of page 1:\")\n",
    "    print(extracted.pages[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3fc2d",
   "metadata": {},
   "source": [
    "## 3. Text Chunking - Split into Searchable Segments\n",
    "\n",
    "Now let's chunk the extracted text with overlap for better search context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9757ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è  Chunking Results:\n",
      "  - Total chunks: 7\n",
      "  - Chunk size: 100 tokens\n",
      "  - Overlap: 20 tokens\n",
      "\n",
      "üì¶ Chunk Details:\n",
      "\n",
      "  Chunk 1:\n",
      "    Page: 1\n",
      "    Tokens: 100\n",
      "    Hash: 8b82db94dd0ba2ba...\n",
      "    Text: #   T e s t   M a r k d o w n   F i l e   T h i s   i s   a   t e s t   m a r k d o w n   f i l e   ...\n",
      "\n",
      "  Chunk 2:\n",
      "    Page: 1\n",
      "    Tokens: 100\n",
      "    Hash: e9a1106dc7b3bd81...\n",
      "    Text: t r o d u c t i o n   T h i s   d o c u m e n t   c o n t a i n s   *   *   f o r m a t t e d   t e ...\n",
      "\n",
      "  Chunk 3:\n",
      "    Page: 1\n",
      "    Tokens: 100\n",
      "    Hash: d3adff138c5750da...\n",
      "    Text: s   .   #   #   S e c t i o n   2   :   J a p a n e s e   C o n t e n t   Êó• Êú¨   Ë™û   „ÅÆ   „Ç≥ „É≥ „ÉÜ „É≥ „ÉÑ   ...\n",
      "\n",
      "  Chunk 4:\n",
      "    Page: 1\n",
      "    Tokens: 100\n",
      "    Hash: 2cd8f9e05010f33e...\n",
      "    Text: „Éî „É• „Éº „Çø   „Åå   „Éá „Éº „Çø   „Åã „Çâ   Â≠¶ Áøí   „Åô „Çã   „Ç¢ „É´ „Ç¥ „É™ „Ç∫ „É†   „Åß „Åô   „ÄÇ   #   #   S e c t i o n   3   :   C o ...\n",
      "\n",
      "  Chunk 5:\n",
      "    Page: 1\n",
      "    Tokens: 100\n",
      "    Hash: ec7921fba9ae7c63...\n",
      "    Text: t   ( \"   H e l l o   ,   R A G   ! \" )   ` ` `   #   #   S e c t i o n   4   :   L i s t s   -   I ...\n",
      "\n",
      "  ... and 2 more chunks\n"
     ]
    }
   ],
   "source": [
    "from src.indexing.chunker import Chunker\n",
    "\n",
    "# Create chunker with smaller chunks for demo\n",
    "chunker = Chunker(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# Chunk the extracted text\n",
    "if extracted:\n",
    "    pages_data = [(page.page_number, page.text) for page in extracted.pages]\n",
    "    chunks = chunker.chunk_pages(pages_data)\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è  Chunking Results:\")\n",
    "    print(f\"  - Total chunks: {len(chunks)}\")\n",
    "    print(f\"  - Chunk size: {chunker.chunk_size} tokens\")\n",
    "    print(f\"  - Overlap: {chunker.chunk_overlap} tokens\")\n",
    "    \n",
    "    print(f\"\\nüì¶ Chunk Details:\")\n",
    "    for i, chunk in enumerate(chunks[:5], 1):  # Show first 5 chunks\n",
    "        print(f\"\\n  Chunk {i}:\")\n",
    "        print(f\"    Page: {chunk.page_number}\")\n",
    "        print(f\"    Tokens: {chunk.token_count}\")\n",
    "        print(f\"    Hash: {chunk.text_hash[:16]}...\")\n",
    "        print(f\"    Text: {chunk.text[:100]}...\")\n",
    "    \n",
    "    if len(chunks) > 5:\n",
    "        print(f\"\\n  ... and {len(chunks) - 5} more chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5c639",
   "metadata": {},
   "source": [
    "## 4. Complete Indexing Pipeline - End-to-End\n",
    "\n",
    "Let's process all pending documents through the complete pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9be65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting indexing pipeline...\n",
      "\n",
      "üìÑ Processing: sample.md\n",
      "  ‚úì Extracted 1 pages, 549 chars\n",
      "  ‚úì Created 1 chunks\n",
      "  ‚úì Status: INDEXED\n",
      "\n",
      "üìÑ Processing: sample.pdf\n",
      "  ‚úì Extracted 3 pages, 424 chars\n",
      "  ‚úì Created 3 chunks\n",
      "  ‚úì Status: INDEXED\n",
      "\n",
      "üìÑ Processing: sample.txt\n",
      "  ‚úì Extracted 1 pages, 486 chars\n",
      "  ‚úì Created 1 chunks\n",
      "  ‚úì Status: INDEXED\n",
      "\n",
      "‚úÖ Pipeline complete! Added 5 chunks to database\n"
     ]
    }
   ],
   "source": [
    "from src.core.models import Chunk\n",
    "\n",
    "# Use standard chunk size for indexing\n",
    "production_chunker = Chunker(chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "print(\"üöÄ Starting indexing pipeline...\\n\")\n",
    "\n",
    "total_chunks_added = 0\n",
    "\n",
    "for doc in pending:\n",
    "    print(f\"üìÑ Processing: {doc.title}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        extracted = extractor.extract(doc.path)\n",
    "        print(f\"  ‚úì Extracted {len(extracted.pages)} pages, {extracted.total_chars} chars\")\n",
    "        \n",
    "        # Step 2: Chunk text\n",
    "        pages_data = [(page.page_number, page.text) for page in extracted.pages]\n",
    "        chunks = production_chunker.chunk_pages(pages_data)\n",
    "        print(f\"  ‚úì Created {len(chunks)} chunks\")\n",
    "        \n",
    "        # Step 3: Add chunks to database\n",
    "        for chunk in chunks:\n",
    "            chunk_obj = Chunk(\n",
    "                id=None,  # Auto-generated\n",
    "                document_id=doc.id,\n",
    "                page=chunk.page_number,\n",
    "                start_offset=chunk.start_offset,\n",
    "                end_offset=chunk.end_offset,\n",
    "                text=chunk.text,\n",
    "                text_hash=chunk.text_hash\n",
    "            )\n",
    "            db.add_chunk(chunk_obj)\n",
    "            total_chunks_added += 1\n",
    "        \n",
    "        # Step 4: Mark as indexed\n",
    "        db.update_document_status(doc.id, DocumentStatus.INDEXED)\n",
    "        print(f\"  ‚úì Status: INDEXED\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        db.update_document_status(doc.id, DocumentStatus.ERROR, str(e))\n",
    "        print(f\"  ‚úó Error: {str(e)}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Pipeline complete! Added {total_chunks_added} chunks to database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006c812",
   "metadata": {},
   "source": [
    "## 5. Verify Indexed Documents\n",
    "\n",
    "Let's check what's now in our database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "573ca12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Database Statistics:\n",
      "\n",
      "Total documents: 4\n",
      "Indexed documents: 4\n",
      "\n",
      "üìã Indexed Documents:\n",
      "\n",
      "  „Éè„É´„Ç∑„Éç„Éº„Ç∑„Éß„É≥.pdf\n",
      "    Type: .pdf\n",
      "    Size: 2,048 bytes\n",
      "    Status: indexed\n",
      "    Chunks: 4\n",
      "    Sample chunk (page 1):\n",
      "      Ê©üÊ¢∞ Â≠¶Áøí „ÅØ ‰∫∫Â∑• Áü•ËÉΩ „ÅÆ ‰∏Ä ÂàÜÈáé „Åß„Åô „ÄÇ „Éá„Éº„Çø „Åã„Çâ „Éë„Çø„Éº„É≥ „Çí Â≠¶Áøí „Åó „Åæ„Åô „ÄÇ...\n",
      "\n",
      "  sample.md\n",
      "    Type: .md\n",
      "    Size: 686 bytes\n",
      "    Status: indexed\n",
      "    Chunks: 1\n",
      "    Sample chunk (page 1):\n",
      "      # T e s t M a r k d o w n F i l e T h i s i s a t e s t m a r k d o w n f i l e f o r m y R A G . # ...\n",
      "\n",
      "  sample.pdf\n",
      "    Type: .pdf\n",
      "    Size: 1,910 bytes\n",
      "    Status: indexed\n",
      "    Chunks: 3\n",
      "    Sample chunk (page 1):\n",
      "      T e s t   P D F   D o c u m e n t \n",
      " P a g e   1 \n",
      " T h i s   i s   a   t e s t   P D F   f i l e   f ...\n",
      "\n",
      "  sample.txt\n",
      "    Type: .txt\n",
      "    Size: 613 bytes\n",
      "    Status: indexed\n",
      "    Chunks: 1\n",
      "    Sample chunk (page 1):\n",
      "      T h i s i s a t e s t t e x t f i l e f o r t h e m y R A G i n d e x i n g s y s t e m . T h i s f ...\n",
      "\n",
      "üì¶ Total chunks in database: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä Database Statistics:\\n\")\n",
    "\n",
    "# Get all documents\n",
    "all_documents = db.get_all_documents()\n",
    "indexed_docs = [d for d in all_documents if d.status == DocumentStatus.INDEXED]\n",
    "\n",
    "print(f\"Total documents: {len(all_documents)}\")\n",
    "print(f\"Indexed documents: {len(indexed_docs)}\")\n",
    "\n",
    "print(f\"\\nüìã Indexed Documents:\")\n",
    "for doc in indexed_docs:\n",
    "    doc_chunks = db.get_chunks_by_document(doc.id)\n",
    "    print(f\"\\n  {doc.title}\")\n",
    "    print(f\"    Type: {doc.ext}\")\n",
    "    print(f\"    Size: {doc.size:,} bytes\")\n",
    "    print(f\"    Status: {doc.status.value}\")\n",
    "    print(f\"    Chunks: {len(doc_chunks)}\")\n",
    "    \n",
    "    # Show a sample chunk\n",
    "    if doc_chunks:\n",
    "        sample = doc_chunks[0]\n",
    "        print(f\"    Sample chunk (page {sample.page}):\")\n",
    "        print(f\"      {sample.text[:100]}...\")\n",
    "\n",
    "# Total chunk count\n",
    "total_chunks = db.get_chunk_count()\n",
    "print(f\"\\nüì¶ Total chunks in database: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a977581",
   "metadata": {},
   "source": [
    "## 6. Search Testing - Ready for Phase 3!\n",
    "\n",
    "The chunks are now indexed and ready to be searched. Phase 3 will add keyword + semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8d2e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Basic FTS5 Search Test:\n",
      "\n",
      "Query: 'markdown' ‚Üí 0 results\n",
      "Query: 'python' ‚Üí 1 results\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'chunk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m ‚Üí \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m results\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results[:\u001b[32m2\u001b[39m], \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m         chunk = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchunk\u001b[49m\n\u001b[32m     12\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk.page\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk.text[:\u001b[32m60\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müí° Note: FTS5 is working! Phase 3 will add:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'chunk'"
     ]
    }
   ],
   "source": [
    "print(\"üîç Basic FTS5 Search Test:\\n\")\n",
    "\n",
    "# Try searching for common words\n",
    "test_queries = [\"markdown\", \"python\", \"file\", \"document\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = db.search_chunks_fts(query, limit=3)\n",
    "    print(f\"Query: '{query}' ‚Üí {len(results)} results\")\n",
    "    \n",
    "    for i, result in enumerate(results[:2], 1):\n",
    "        chunk = result.chunk\n",
    "        print(f\"  {i}. Page {chunk.page}: {chunk.text[:60]}...\")\n",
    "\n",
    "print(f\"\\nüí° Note: FTS5 is working! Phase 3 will add:\")\n",
    "print(\"  - Proper query tokenization\")\n",
    "print(\"  - Semantic search with embeddings\")\n",
    "print(\"  - Hybrid search (keyword + semantic)\")\n",
    "print(\"  - Re-ranking of results\")\n",
    "print(\"  - RAG answer generation with citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6243c51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 2 Summary\n",
    "\n",
    "You've just seen the complete file indexing pipeline:\n",
    "\n",
    "1. **Ingestion**: Scanned folders and found 3 documents (PDF, TXT, MD)\n",
    "2. **Extraction**: Extracted text with page numbers preserved\n",
    "3. **Chunking**: Split text into overlapping chunks with Japanese tokenization\n",
    "4. **Storage**: Added chunks to database with automatic FTS5 indexing\n",
    "5. **Verification**: All documents successfully indexed\n",
    "\n",
    "**Test Coverage**: 69 tests passing (33 Phase 1 + 36 Phase 2)\n",
    "\n",
    "**Next**: Phase 3 will implement search and RAG answer generation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
